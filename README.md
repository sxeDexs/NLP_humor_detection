# Автоматический анализ юмора (Сарказм/Шутки) в русском тексте (BERT vs RNN)

Этот проект был выполнен в рамках Семинара по обработке естественных языков (ОЕЯ) кафедры алгоритмической математики.

**Цель проекта:** Исследовать возможность автоматического обнаружения юмора (включая сарказм и шутки) в русскоязычных текстах с использованием современных моделей NLP. В рамках работы реализованы и сравнены два подхода: на основе архитектуры Transformer (BERT) и Рекуррентной Нейронной Сети (RNN/GRU). Дополнительно создан простой GUI-инструмент для визуальной демонстрации работы моделей.

## Описание

Этот проект посвящен разработке и исследованию системы автоматического обнаружения юмора, в частности сарказма и шуток, в русскоязычных текстах. В рамках проекта реализованы два основных модуля:

1.  **`main.py`**: Консольное приложение для полного цикла работы с моделями:
    *   Сборка и подготовка датасета из различных источников.
    *   Обучение двух моделей классификации: **BERT** (на базе `rubert-tiny2`) и **RNN** (GRU с эмбеддингами **Navec**).
    *   Тестирование и оценка производительности моделей.
2.  **`sarcasm_gui.py`**: Графический интерфейс пользователя (GUI) на основе `tkinter`, позволяющий:
    *   Загружать или вставлять текст.
    *   Выбирать обученную модель (BERT или RNN).
    *   Запускать анализ и **визуально подсвечивать** предложения, распознанные моделью как юмористические.

Цель проекта — изучить и сравнить подходы к обнаружению юмора с использованием современных NLP-моделей и предоставить инструмент для интерактивного анализа текста.

## Содержание

*   [Подготовка Датасета](#подготовка-датасета)
*   [Архитектуры Моделей](#архитектуры-моделей)
    *   [BERT (Transformer)](#bert-transformer)
    *   [RNN (GRU с Navec)](#rnn-gru-с-navec)
*   [Обучение и Оценка](#обучение-и-оценка)
*   [Требования и Установка](#требования-и-установка)
*   [Использование](#использование)
    *   [Работа с `main.py`](#работа-с-mainpy)
    *   [Работа с `sarcasm_gui.py`](#работа-с-sarcasm_guipy)
*   [Алгоритм Подсветки в GUI](#алгоритм-подсветки-в-gui)
*   [Полезные Ссылки](#полезные-ссылки)

## Подготовка Датасета

Качество данных критически важно для обучения моделей NLP. Датасет для этого проекта формируется следующим образом (`main.py`, действия 1-3):

1.  **Перевод англоязычного сарказма:**
    *   **Источник:** Используется англоязычный датасет с метками сарказма (например, [News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) или аналогичный файл `train-balanced-sarcasm.csv`).
    *   **Процесс:** Тексты переводятся на русский язык с помощью библиотеки `googletrans`. **ВНИМАНИЕ:** Этот процесс может быть **очень долгим** и зависит от API Google Translate. Результат сохраняется в `dataset/train_ru.csv`. Качество автоматического перевода может влиять на модель.
2.  **Подготовка русских шуток:**
    *   **Источник:** CSV-файл с русскими шутками (`dataset/jokes.csv`), содержащий колонку с текстом (`text` или `comment`).
    *   **Процесс:** Удаляются ненужные колонки, текст очищается, всем записям присваивается метка юмора (1). Результат сохраняется в `dataset/train_ru_jokes.csv`.
3.  **Формирование финального датасета:**
    *   **Объединение:** Выбирается заданное количество не-саркастических (метка 0) и саркастических (метка 1) примеров из переведенного датасета, а также шуток (метка 1). Все объединяется в один датафрейм.
    *   **Очистка:** Тексты приводятся к нижнему регистру, удаляются пунктуация и спецсимволы, лишние пробелы. Пустые строки удаляются.
    *   **Разделение:** Итоговый датасет перемешивается и делится на обучающую (`dataset/train_final.csv`) и тестовую (`dataset/test_final.csv`) выборки со стратификацией по меткам.

## Архитектуры Моделей

В проекте реализованы две модели для бинарной классификации текста (юмор/не юмор).

### BERT (Transformer)

*   **Основа:** Используется предобученная на русском языке модель `cointegrated/rubert-tiny2` – компактный вариант BERT.
*   **Архитектура:** Модель дообучается для задачи классификации последовательностей (`BertForSequenceClassification`). Поверх базовой модели добавляется классификационный слой.
*   **Входные данные:** Текст токенизируется с помощью `BertTokenizer`, добавляются служебные токены `[CLS]` и `[SEP]`, длина последовательности унифицируется паддингом или обрезкой до `max_len=512`.
*   **Обучение:** Оптимизатор AdamW, функция потерь CrossEntropyLoss, линейный планировщик скорости обучения.
*   **Результат:** Обученная модель сохраняется в `dataset/models/bert_classifier.pt`.

### RNN (GRU с Navec)

*   **Основа:** Рекуррентная нейронная сеть на основе **GRU** (Gated Recurrent Unit).
*   **Эмбеддинги:** Для представления слов используются предобученные векторные представления **Navec** от проекта Natasha ([ссылка на скачивание](https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar)). Это позволяет модели использовать семантическую информацию о словах. Эмбеддинги для слов из Navec загружаются в `nn.Embedding` слой и могут быть "заморожены" во время обучения.
*   **Токенизация и Словарь:**
    *   `TokenizerRNN`: Простой токенизатор на основе регулярного выражения.
    *   `VocabRNN`: Создает словарь на основе частотности токенов в обучающей выборке, ограничивая его размер (`max_vocab_size`). Добавляет специальные токены `<PAD>`, `<UNK>`, `<EOS>`.
*   **Архитектура:** Несколько слоев двунаправленного GRU (`bidirectional=True`). Выход последнего скрытого состояния GRU подается на полносвязные слои с ReLU и Dropout для финальной классификации.
*   **Обработка последовательностей:** Используется `torch.nn.utils.rnn.pack_sequence` для эффективной обработки батчей с текстами разной длины.
*   **Результат:** Сохраняются модель (`dataset/models/rnn_model.pt`), словарь (`dataset/models/rnn_vocab.joblib`) и токенизатор (`dataset/models/rnn_tokenizer.joblib`).

## Обучение и Оценка

Процесс обучения и оценки моделей (`main.py`, действия 4-5 и тестирование в конце обучения):

1.  **Подготовка:** Обучающий датасет (`train_final.csv`) делится на обучающую и валидационную подвыборки.
2.  **Обучение:** Модель обучается на обучающей подвыборке в течение заданного числа эпох. После каждой эпохи модель оценивается на валидационной подвыборке. Сохраняется версия модели, показавшая лучший результат на валидации (например, по F1-score или accuracy).
3.  **Тестирование:** Лучшая сохраненная модель оценивается на отложенной тестовой выборке (`test_final.csv`), которая не участвовала в обучении.
4.  **Метрики:** Рассчитываются стандартные метрики бинарной классификации:
    *   Accuracy (Точность)
    *   Recall (Полнота)
    *   F1-score
5.  **Визуализация:** Строится и сохраняется матрица ошибок (Confusion Matrix), показывающая эффективность модели на каждом классе (`dataset/models/bert_confusion_matrix.png`, `dataset/models/rnn_confusion_matrix.png`).

## Требования и Установка

1.  **Python:** Версия 3.8 или выше.
2.  **Основные библиотеки:** `torch`, `transformers`, `scikit-learn`, `pandas`, `numpy`, `nltk`, `joblib`, `seaborn`, `matplotlib`, `tqdm`.
3.  **Для RNN:** `navec`.
4.  **Для перевода:** `googletrans==4.0.0-rc1` (или другая рабочая версия), `asyncio`.
5.  **Для GUI:** `tkinter` (обычно включен в Python), `razdel` (рекомендуется для лучшего разбиения на предложения), `ttkbootstrap` (опционально, для улучшения внешнего вида).

**Рекомендуется создать виртуальное окружение:**

```bash
python -m venv venv
# Linux/macOS
source venv/bin/activate
# Windows
venv\Scripts\activate


**Установите зависимости:**
(Создайте файл `requirements.txt` со списком библиотек или установите их вручную)

```bash
pip install torch transformers scikit-learn pandas numpy nltk joblib seaborn matplotlib tqdm navec googletrans==4.0.0-rc1 razdel ttkbootstrap
```

**Скачайте Navec:**
*   Загрузите файл `navec_hudlit_v1_12B_500K_300d_100q.tar` по [этой ссылке](https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar).
*   Поместите скачанный файл в папку `dataset/`.

**Загрузите ресурсы NLTK (для GUI, если `razdel` не установлен):**

```python
import nltk
nltk.download('punkt')
```

## Использование

### Работа с `main.py`

Запустите скрипт из командной строки:

```bash
python main.py
```

Вам будет предложено меню с действиями:

```
==================================================
Доступные действия:
  1: Перевести английский датасет (ОЧЕНЬ ДОЛГО!)
  2: Подготовить CSV файл с шутками
  3: Создать финальный датасет (train/test)
  4: Обучить Transformer (BERT)
  5: Обучить RNN
  6: Предсказать с помощью Transformer (BERT)
  7: Предсказать с помощью RNN
  0: Выход
==================================================
Введите номер действия:
```

*   **1-3:** Последовательно выполните эти шаги для подготовки данных. Шаг 1 может запросить количество строк для перевода.
*   **4, 5:** Запустите обучение выбранной модели. Убедитесь, что данные подготовлены (шаги 1-3) и файл Navec находится в `dataset/`.
*   **6, 7:** Используйте обученную модель для предсказания метки юмора для введенного вами текста.
*   **0:** Выход.

### Работа с `sarcasm_gui.py`

Запустите графический интерфейс:

```bash
python sarcasm_gui.py
```

**Интерфейс:**

*   **Текстовое поле:** Вставьте сюда текст для анализа (Ctrl+V работает) или откройте файл.
*   **Кнопка "Открыть файл":** Загружает текст из `.txt` файла (UTF-8).
*   **Выпадающий список "Backend":** Выберите, какую модель использовать для анализа ('transformer' или 'rnn'). Модель должна быть предварительно обучена с помощью `main.py`.
*   **Кнопка "Найти юмор":** Запускает процесс анализа.
*   **Индикатор прогресса:** Показывает, что идет обработка.
*   **Результат:** Предложения, классифицированные как юмор (метка 1), будут подсвечены зеленым фоном в текстовом поле.

## Алгоритм Подсветки в GUI

Когда вы нажимаете "Найти юмор" в `sarcasm_gui.py`, происходит следующее:

1.  **Извлечение текста:** Текст берется из виджета `tk.Text`.
2.  **Сегментация на предложения:** Текст делится на предложения. Используется библиотека `razdel` (если доступна, она лучше подходит для русского языка), либо `nltk.tokenize.sent_tokenize` в качестве запасного варианта.
3.  **Расчет смещений:** Для каждого предложения определяются его начальный и конечный индексы символов в исходном тексте. Это нужно для точной подсветки.
4.  **Загрузка модели:** Класс `SentenceClassifier` лениво загружает веса выбранной модели (BERT или RNN), если они еще не были загружены.
5.  **Предсказание для каждого предложения:** Модель (`predict_fn`) вызывается для **каждого** предложения по отдельности.
6.  **Применение тегов:** Если модель предсказывает для предложения метку 1 (юмор), к соответствующему диапазону символов (от начального до конечного смещения) в виджете `tk.Text` применяется тег `'humor'`, который настроен на зеленую подсветку фона.

## Полезные Ссылки

*   **Navec Embeddings:** [Natasha Project](https://github.com/natasha/navec) (Файл: [navec_hudlit_v1_12B_500K_300d_100q.tar](https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar))
*   **Пример датасета сарказма:** [Kaggle - News Headlines Dataset for Sarcasm Detection](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection)
*   **Hugging Face Transformers:** [Библиотека Transformers](https://huggingface.co/docs/transformers/index)
*   **Razdel (сегментация):** [Библиотека Razdel](https://github.com/natasha/razdel)
*   **PyTorch:** [Официальный сайт PyTorch](https://pytorch.org/)
